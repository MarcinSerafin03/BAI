{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "bsnd8ck9-JhU",
        "tags": []
      },
      "source": [
        "# Przetwarzanie języka naturalnego\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5uQlTHTHN4r"
      },
      "source": [
        "## Wstęp\n",
        "\n",
        "Obecnie najpopularniejsze model służące do przetwarzania języka naturalnego wykorzystują architekturę transformacyjną. Istnieje kilka bibliotek, implementujących tę architekturę, ale w kontekście NLP najczęściej wykorzystuje się [Huggingface transformers](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "Biblioteka ta poza samym [kodem źródłowym](https://github.com/huggingface/transformers), zawiera szereg innych elementów. Do najważniejszych z nich należą:\n",
        "* [modele](https://huggingface.co/models) - olbrzymia i ciągle rosnąca liczba gotowych modeli, których możemy użyć do rozwiązywania wielu problemów z dziedziny NLP (ale również w zakresie rozpoznawania mowy, czy przetwarzania obrazu),\n",
        "* [zbiory danych](https://huggingface.co/datasets) - bardzo duży katalog przydatnych zbiorów danych, które możemy w prosty sposób wykorzystać do trenowania własnych modeli NLP (oraz innych modeli)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "JCVKT9diUlqT",
        "tags": []
      },
      "source": [
        "## Weryfikacja dostępności GPU\n",
        "\n",
        "Trening modeli NLP wymaga dostępu do akceleratorów sprzętowych, przyspieszających uczenie sieci neuronowych. Jeśli nasz komputer nie jest wyposażony w GPU, to możemy skorzystać ze środowiska Google Colab.\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apohllo/sztuczna-inteligencja/blob/master/lab5/lab_5.ipynb)\n",
        "\n",
        "W tym środowisku możemy wybrać akcelerator spośród GPU i TPU.\n",
        "\n",
        "Sprawdźmy, czy mamy dostęp do środowiska wyposażonego w akcelerator NVidii:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8OgLsVgK0bK",
        "outputId": "3b4a0ae9-d25f-4247-d1e0-291062bcc04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan  1 16:36:36 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0              33W /  70W |    623MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iHWHwumLJy-"
      },
      "source": [
        "Jeśli akcelerator jest niedostępny (polecenie skończyło się błędem), to zmieniamy środowisko wykonawcze wybierając z menu \"Środowisko wykonawcze\" -> \"Zmień typ środowiska wykonawczego\" -> GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTXP41EDFoA4"
      },
      "source": [
        "## Podpięcie dysku Google (opcjonalne)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXbT070FoA4"
      },
      "source": [
        "Kolejnym elementem przygotowań, który jest opcjonalny, jest dołączenie własnego dysku Google Drive do środowiska Colab. Dzięki temu możliwe jest zapisywanie wytrenowanych modeli, w trakcie procesu treningu, na \"zewnętrznym\" dysku. Jeśli Google Colab doprowadzi do przerwania procesu treningu, to mimo wszystko pliki, które udało się zapisać w trakcie treningu nie przepadną. Możliwe będzie wznowienie treningu już na częściowo wytrenowanym modelu.\n",
        "\n",
        "W tym celu montujemy dysk Google w Colabie. Wymaga to autoryzacji narzędzia Colab w Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-13T11:01:10.422451Z",
          "start_time": "2023-01-13T11:01:09.790725Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ysEoT8AhFoA4",
        "outputId": "a41fa315-392c-40f3-e10f-41838104d485"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4996ee3d8d09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grytPGtiFoA4"
      },
      "source": [
        "Po podmontowaniu dysku mamy dostęp do całej zawartości Google Drive. Wskazując miejsce zapisywania danych w trakcie treningu należy wskazać ścieżkę zaczynającą się od `/content/gdrive`, ale należy wskazać jakiś podkatalog w ramach naszej przestrzeni dyskowej. Pełna ścieżka może mieć postać `/content/gdrive/MyDrive/output`. Przed uruchomieniem treningu warto sprawdzić, czy dane zapisują się na dysku."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubd7LV7kI3wo"
      },
      "source": [
        "## Instalacja bibliotek Pythona"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_GoQx_K6sC"
      },
      "source": [
        "Podobnie jak w poprzednich laboratoriach optymalnym sposobem instalacji bibliotek jest wykorzystanie narzędzia Poetry, które ma ustalone wersji bibliotek w pliku `poetry.lock`. Biblioteki te zostały zmodyfikowane względem wcześniejszych laboratoriów, dlatego ponownie powinniśmy jest zainstalować."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeJtMsvBJ48f",
        "outputId": "7bbd9667-9984-4e29-f6f6-78ff6404ed5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: poetry: command not found\n"
          ]
        }
      ],
      "source": [
        "!poetry install --no-root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJunO6pV_tRK"
      },
      "source": [
        "Mając zainstalowane niezbedne bilioteki, możemy skorzystać z wszystkich modeli i zbiorów danych zarejestrowanych w katalogu.\n",
        "\n",
        "Typowym sposobem użycia dostępnych modeli jest:\n",
        "* *wykorzystanie gotowego modelu*, który realizuje określone zadanie, np. [analizę senetymentu w języku angielskim](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis) - model tego rodzaju nie musi być trenowywany, wystarczy go uruchomić aby uzyskać wynik klasyfikacji (można to zobaczyć w demo pod wskazanym linkiem),\n",
        "* *wykorzystanie modelu bazowego*, który jest dotrenowywany do określonego zadania; przykładem takiego modelu jest [HerBERT base](https://huggingface.co/allegro/herbert-base-cased), który uczony był jako maskowany model języka. Żeby wykorzystać go do konkretnego zadania, musimy wybrać dla niego \"głowę klasyfikacyjną\" oraz dotrenować na własnym zbiorze danych.\n",
        "\n",
        "Modele tego rodzaju różnią się od siebie, można je załadować za pomocą wspólnego interfejsu, ale najlepiej jest wykorzystać jedną ze specjalizowanych klas, dostosowanych do zadania, które chcemy zrealizować. Zaczniemy od załadowania modelu BERT base - jednego z najbardziej popularnych modeli, dla języka angielskiego. Za jego pomocą będziemy odgadywać brakujące wyrazy w tekście. Wykorzystamy do tego wywołanie `AutoModelForMaskedLM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "wTCDkZ1nKIEm",
        "outputId": "eb864d0e-da9c-4ac4-81a9-4c3a6433bd14",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "RCHU5ArMJZfC",
        "tags": []
      },
      "source": [
        "Załadowany model jest modułem PyTorcha. Możemy zatem korzystać z API tej biblioteki. Możemy np. sprawdzić ile parametrów ma model BERT base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "editable": true,
        "id": "M-dS04e4JX4x",
        "outputId": "062a84ea-65b5-4baf-891e-6f8c72825dae",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'108 340 804'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "'{:,}'.format(count).replace(',', ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "S9aPClBxKEWq",
        "tags": []
      },
      "source": [
        "Widzimi zatem, że nasz model jest bardzo duży - zawiera ponad 100 milionów parametrów, a jest to tzw. model bazowy. Modele obecnie wykorzystywane mają jeszcze więcej parametrów - duże modele językowe, takie jak ChatGPT posiadają więcej niż 100 miliardów parametrów.\n",
        "\n",
        "Możemy również podejrzeć samą strukturę modelu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "TqCH8YrzKguC",
        "outputId": "f3c68da2-d4ff-463a-aae4-81e573691496",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3LlBdmpqsElx"
      },
      "source": [
        "Jeśli dysponujemy akceleratorem (GPU lub inny), to pamiętajmy żeby przeniść model na ten akcelerator, np."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "TBN9Z9rDsEly",
        "outputId": "0318ad2e-0b68-4617-d42b-f26ed34dacdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Jeśli chcesz użyć akceleratora wpisz \"cuda:0\" lub nazwę odpowiedniego akceleratora.\n",
        "# Żeby kod poniżej dział, ale obliczenia były wykonywane na CPU, wpisz \"cpu\"\n",
        "#device = \"cpu\"\n",
        "device = \"cuda:0\"\n",
        "model.to(device)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "gdgyGz752126",
        "tags": []
      },
      "source": [
        "# Tokenizacja tekstu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "zmX8eu_mB9CO",
        "tags": []
      },
      "source": [
        "Załadowanie samego modelu nie jest jednak wystarczające, żeby zacząć go wykorzystywać. Musimy mieć mechanizm zamiany tekstu (łańcucha znaków), na ciąg tokenów, należących do określonego słownika. W trakcie treningu modelu, słownik ten jest określany (wybierany w sposób algorytmiczny) przed właściwym treningiem sieci neuronowej. Choć możliwe jest jego późniejsze rozszerzenie (douczenie na danych treningowych, pozwala również uzyskać reprezentację brakujących tokenów), to zwykle wykorzystuje się słownik w postaci, która została określona przed treningiem sieci neuronowej. Dlatego tak istotne jest wskazanie właściwego słownika dla tokenizera dokonującego podziału tekstu.\n",
        "\n",
        "Biblioteka posiada klasę `AutoTokenizer`, która akceptuje nazwę modelu, co pozwala automatycznie załadować słownik korespondujący z wybranym modelem sieci neuronowej. Trzeba jednak pamiętać, że jeśli używamy 2 modeli, to każdy z nich najpewniej będzie miał inny słownik, a co za tym idzie muszą one mieć własne instancje klasy `Tokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "PYUsVa1fBTPW",
        "outputId": "f8b1eb8f-8434-4e59-872d-2dab42e7a3e0",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "rXIePLylEFx2",
        "tags": []
      },
      "source": [
        "Tokenizer posługuje się słownikiem o stałym rozmiarze. Podowuje to oczywiście, że nie wszystkie wyrazy występujące w tekście, będą się w nim znajdowały. Co więcej, jeśli użyjemy tokenizera do podziału tekstu w innym języku, niż ten dla którego został on stworzony, to taki tekst będzie dzielony na większą liczbę tokenów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "DAGb1Jzhtr9p",
        "outputId": "122e7127-4679-44ad-e99a-2960ac6d7970",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1109,  3613,  3058, 17594, 15457,  1166,  1103, 16688,  3676,\n",
            "           119,   102]])\n",
            "torch.Size([1, 12])\n",
            "tensor([[  101,   163,  1161, 28259,  7774, 20671,  7128,   176, 28221, 28244,\n",
            "          1233, 28213,   179,  1161, 28257, 19339,   119,   102]])\n",
            "torch.Size([1, 18])\n"
          ]
        }
      ],
      "source": [
        "sentence1 = tokenizer.encode(\n",
        "    \"The quick brown fox jumps over the lazy dog.\", return_tensors=\"pt\"\n",
        ")\n",
        "print(sentence1)\n",
        "print(sentence1.shape)\n",
        "\n",
        "sentence2 = tokenizer.encode(\"Zażółć gęślą jaźń.\", return_tensors=\"pt\")\n",
        "print(sentence2)\n",
        "print(sentence2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "2ILQRogoErrt",
        "tags": []
      },
      "source": [
        "Korzystająć z tokenizera dla języka angielsiego do podziału polskiego zdania, widzimy, że otrzymujemy znacznie większą liczbę tokenów. Żeby zobaczyć, w jaki sposób tokenizer dokonał podziału tekstu, możemy wykorzystać wywołanie `covert_ids_to_tokens`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "DOnw6mq81QFg",
        "outputId": "97c855d7-67c9-4d47-fd64-eea2ebd08995",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]|The|quick|brown|fox|jumps|over|the|lazy|dog|.|[SEP]\n",
            "[CLS]|Z|##a|##ż|##ó|##ł|##ć|g|##ę|##ś|##l|##ą|j|##a|##ź|##ń|.|[SEP]\n"
          ]
        }
      ],
      "source": [
        "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence1[0]))))\n",
        "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence2[0]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "VZzt3-w5GQDB",
        "tags": []
      },
      "source": [
        "Widzimy, że dla jęzka angielskiego wszystkie wyrazy w zdaniu zostały przekształcone w pojedyncze tokeny. W przypadku zdania w języku polskim, zawierającego szereg znaków diakrytycznych sytuacja jest zupełnie inna - każdy znak został wyodrębniony do osobnego sub-tokenu. To, że mamy do czynienia z sub-tokenami sygnalizowane jest przez dwa krzyżyki poprzedzające dany sub-token. Oznaczają one, że ten sub-token musi być sklejony z porzedzającym go tokenem, aby uzyskać właściwy łańcuch znaków."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "Q_aJHTzvsEl2"
      },
      "source": [
        "## Zadanie 1 (0.5 punkt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "b6OtR0FMsEl3"
      },
      "source": [
        "Wykorzystaj tokenizer dla modelu `allegro/herbert-base-cased`, aby dokonać tokenizacji tych samych zdań. Jakie wnioski można wyciągnąć przyglądając się sposobowi tokenizacji za pomocą różnych słowników?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-12-20T13:58:00.312979Z",
          "start_time": "2022-12-20T13:58:00.303639Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "qEir3EhlHHaQ",
        "outputId": "334dd494-01f1-45c6-dd7e-7a47d8dc9289",
        "tags": [
          "ex"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,  7117, 22991,  4879, 25015,  1016,  3435,  1055,  2202,  4952,\n",
            "          1010,    83, 10259,  6854,  2050,  3852,  2065,  1031,  1899,     2]])\n",
            "torch.Size([1, 20])\n",
            "tensor([[    0,  2237,  7227,  1048,  7029, 46389,  2059,   272,  1059,  1899,\n",
            "             2]])\n",
            "torch.Size([1, 11])\n",
            "<s>|The</w>|qui|ck</w>|brow|n</w>|fo|x</w>|ju|mp|s</w>|o|ver</w>|the</w>|la|zy</w>|do|g</w>|.</w>|</s>\n",
            "<s>|Za|żół|ć</w>|gę|ślą</w>|ja|ź|ń</w>|.</w>|</s>\n"
          ]
        }
      ],
      "source": [
        "# your_code\n",
        "# !pip install sacremoses\n",
        "\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "sentence1 = tokenizer2.encode(\n",
        "    \"The quick brown fox jumps over the lazy dog.\", return_tensors=\"pt\"\n",
        ")\n",
        "print(sentence1)\n",
        "print(sentence1.shape)\n",
        "\n",
        "sentence2 = tokenizer2.encode(\"Zażółć gęślą jaźń.\", return_tensors=\"pt\")\n",
        "print(sentence2)\n",
        "print(sentence2.shape)\n",
        "\n",
        "\n",
        "print(\"|\".join(tokenizer2.convert_ids_to_tokens(list(sentence1[0]))))\n",
        "print(\"|\".join(tokenizer2.convert_ids_to_tokens(list(sentence2[0]))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "US-hA9UMOPk_",
        "tags": [
          "ex"
        ]
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "HJquTQTDHLQY",
        "tags": []
      },
      "source": [
        "W wynikach tokenizacji poza wyrazami/tokenami występującymi w oryginalnym tekście pojawiają się jeszcze dodatkowe znaczniki `[CLS]` oraz `[SEP]` (albo inne znaczniki - w zależności od użytego słownika). Mają one specjalne znaczenie i mogą być wykorzystywane do realizacji specyficznych funkcji związanych z analizą tekstu. Np. reprezentacja tokenu `[CLS]` wykorzystywana jest w zadaniach klasyfikacji zdań. Z kolei token `[SEP]` wykorzystywany jest do odróżnienia zdań, w zadaniach wymagających na wejściu dwóch zdań (np. określenia, na ile zdania te są podobne do siebie).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "tFR6OfWBU0TP",
        "tags": []
      },
      "source": [
        "# Modelowanie języka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "F2dVbEVuOoy1",
        "tags": []
      },
      "source": [
        "Modele pretrenowane w reżimie self-supervised learning (SSL) nie posiadają specjalnych zdolności w zakresie rozwiązywania konkretnych zadań z zakresu przetwarzania języka naturalnego, takich jak odpowiadanie na pytania, czy klasyfikacja tekstu (z wyjątkiem bardzo dużych modeli, takich jak np. GPT-3, których model językowy zdolny jest do predykcji np. sensownych odpowiedzi na pytania). Można je jednak wykorzystać do określania prawdopodobieństwa wyrazów w tekście, a tym samym do sprawdzenia, jaką wiedzę posiada określony model w zakresie znajomości języka, czy też ogólną wiedzę o świecie.\n",
        "\n",
        "Aby sprawdzić jak model radzi sobie w tych zadaniach, możemy dokonać inferencji na danych wejściowych, w których niektóre wyrazy zostaną zastąpione specjalnymi symbolami maskującymi, wykorzystywanymi w trakcie pre-treningu modelu.\n",
        "\n",
        "Należy mieć na uwadze, że różne modele mogą korzystać z różnych specjalnych sekwencji w trakcie pretreningu. Np. Bert korzysta z sekwencji `[MASK]`. Wygląd tokenu maskującego lub jego identyfikator możemy sprawdzić w [pliku konfiguracji tokenizera](https://huggingface.co/bert-base-cased/raw/main/tokenizer.json) dystrubowanym razem z modelem, albo odczytać wprost z instancji tokenizera.\n",
        "\n",
        "W pierwszej kolejności, spróbujemy uzupełnić brakujący wyraz w angielskim zdaniu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "YgV2T4C3xsaD",
        "outputId": "136c61d2-3086-4c32-bf65-dbe7f07f2731",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]|The|quick|brown|[MASK]|jumps|over|the|lazy|dog|.|[SEP]\n",
            "tensor([-5.3489, -5.6063, -5.1303,  ..., -5.9625, -4.1559, -4.5403],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "sentence_en_text = \"The quick brown [MASK] jumps over the lazy dog.\"\n",
        "\n",
        "sentence_en = tokenizer.encode(\n",
        "    sentence_en_text, return_tensors=\"pt\"\n",
        ")\n",
        "#\n",
        "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence_en[0]))))\n",
        "target = model(sentence_en.to(device))\n",
        "print(target.logits[0][4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "zc5CfCfSRV5E",
        "tags": []
      },
      "source": [
        "Ponieważ zdanie po stokenizowaniu uzupełniane jest znacznikiem `[CLS]`, to zamaskowane słowo znajduje się na 4 pozycji. Wywołanie `target.logits[0][4]` pokazuje tensor z rozkładem prawdopodobieństwa poszczególnych wyrazów, które zostało określone na podstawie parametrów modelu. Możemy wybrać wyrazy, które posiadają największe prawdopodobieństwo, korzystając z wywołania `torch.topk`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "C3ugmBzhz5uu",
        "outputId": "ef406ff9-9d26-44ca-99f2-6270f2d4ca47",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([12.1982, 11.2289, 10.6009, 10.1278, 10.0120], device='cuda:0',\n",
              "       grad_fn=<TopkBackward0>),\n",
              "indices=tensor([ 3676,  1663,  5855,  4965, 21566], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "top = torch.topk(target.logits[0][4], 5)\n",
        "top"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "Xz5nw1LbR5Va",
        "tags": []
      },
      "source": [
        "Otrzymaliśmy dwa wektory - `values` zawierający składowe wektora wyjściowego sieci neuronowej (nieznormalizowane) oraz `indices` zawierający indeksy tych składowych. Na tej podstawie możemy wyświetlić wyraz, które według modelu są najbardziej prawdopodobnymi uzupełnieniami zamaskowanego wyrazu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "editable": true,
        "id": "kkZKTw0J2BUn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "words = tokenizer.convert_ids_to_tokens(top.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "editable": true,
        "id": "kmDVEzZQ2Omz",
        "outputId": "00327a8a-6d24-4542-9d3b-4f7b1cb740c3",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 5 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf/0lEQVR4nO3deXRU9f3/8VdIyBAmyUBAAkhCUFBDWEVBwbIUFBEFPV3EQkFaFhXKZhXSgogIQXBBKoVKT4VWNmsFrWsV2ZewBBArEKIRUllVTCDIEJL37w+/zK+RoGBnPpPA83HOnOPcezOfz73AzNM7NzMRZmYCAABwpFK4JwAAAC4txAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcigr3BL6tpKRE+/fvV1xcnCIiIsI9HQAAcB7MTMeOHVPdunVVqdJ3n9sod/Gxf/9+JSUlhXsaAADgB8jLy1O9evW+c5tyFx9xcXGSvpl8fHx8mGcDAADOR0FBgZKSkgKv49+l3MXHmbda4uPjiQ8AACqY87lkggtOAQCAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeiwj0B11LGvBHuKVQYn07pHu4pAAAuQpz5AAAATl1wfKxatUp33HGH6tatq4iICC1dujSwrqioSKNHj1bTpk3l9XpVt25d9e3bV/v37w/mnAEAQAV2wfFRWFio5s2ba+bMmWetO3HihLKysjRu3DhlZWXplVde0e7du9WjR4+gTBYAAFR8F3zNR7du3dStW7cy1/l8Pr377rullj333HNq3bq19u3bp+Tk5B82SwAAcNEI+QWn+fn5ioiIULVq1cpc7/f75ff7A/cLCgpCPSUAABBGIb3g9OTJkxo9erTuuecexcfHl7lNRkaGfD5f4JaUlBTKKQEAgDALWXwUFRXp5z//ucxMs2bNOud26enpys/PD9zy8vJCNSUAAFAOhORtlzPhsXfvXr3//vvnPOshSR6PRx6PJxTTAAAA5VDQ4+NMeOzZs0fLly9XjRo1gj0EAACowC44Po4fP66cnJzA/dzcXG3btk0JCQmqU6eOfvrTnyorK0uvv/66iouLdfDgQUlSQkKCoqOjgzdzAABQIV1wfGzevFmdOnUK3B81apQkqV+/fnr00Uf12muvSZJatGhR6ueWL1+ujh07/vCZAgCAi8IFx0fHjh1lZudc/13rAAAA+G4XAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOBUV7gng0pAy5o1wT6HC+HRK93BPAQBCijMfAADAKeIDAAA4RXwAAACniA8AAOAU8QEAAJwiPgAAgFPEBwAAcIr4AAAAThEfAADAKeIDAAA4RXwAAACniA8AAOAU8QEAAJwiPgAAgFPEBwAAcIr4AAAAThEfAADAqahwTwBA6KSMeSPcU6gwPp3SPdxTAC4ZnPkAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4NQFf6vtqlWrNG3aNG3ZskUHDhzQkiVLdOeddwbWm5nGjx+vOXPm6KuvvlK7du00a9YsNWrUKJjzBoByi28TPn98m/Cl6YLPfBQWFqp58+aaOXNmmeunTp2qGTNmaPbs2crMzJTX61XXrl118uTJ/3myAACg4rvgMx/dunVTt27dylxnZpo+fbrGjh2rnj17SpL++te/KjExUUuXLlWvXr3+t9kCAIAK74Lj47vk5ubq4MGD6tKlS2CZz+dTmzZttH79+jLjw+/3y+/3B+4XFBQEc0oAgEsEb3edv3C/3RXUC04PHjwoSUpMTCy1PDExMbDu2zIyMuTz+QK3pKSkYE4JAACUM2H/bZf09HTl5+cHbnl5eeGeEgAACKGgxkft2rUlSYcOHSq1/NChQ4F13+bxeBQfH1/qBgAALl5BjY8GDRqodu3aWrZsWWBZQUGBMjMzdeONNwZzKAAAUEFd8AWnx48fV05OTuB+bm6utm3bpoSEBCUnJ2vEiBF6/PHH1ahRIzVo0EDjxo1T3bp1S30WCAAAuHRdcHxs3rxZnTp1CtwfNWqUJKlfv36aO3euHn74YRUWFmrQoEH66quvdNNNN+ntt99WlSpVgjdrAABQYV1wfHTs2FFmds71EREReuyxx/TYY4/9TxMDAAAXp7D/tgsAALi0EB8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnAp6fBQXF2vcuHFq0KCBYmJidOWVV2rixIkys2APBQAAKqCoYD/gE088oVmzZmnevHlKS0vT5s2b1b9/f/l8Pg0bNizYwwEAgAom6PGxbt069ezZU927d5ckpaSkaOHChdq4cWOwhwIAABVQ0N92adu2rZYtW6bs7GxJ0vbt27VmzRp169atzO39fr8KCgpK3QAAwMUr6Gc+xowZo4KCAl1zzTWKjIxUcXGxJk2apN69e5e5fUZGhiZMmBDsaQAAgHIq6Gc+XnrpJc2fP18LFixQVlaW5s2bpyeffFLz5s0rc/v09HTl5+cHbnl5ecGeEgAAKEeCfubjoYce0pgxY9SrVy9JUtOmTbV3715lZGSoX79+Z23v8Xjk8XiCPQ0AAFBOBf3Mx4kTJ1SpUumHjYyMVElJSbCHAgAAFVDQz3zccccdmjRpkpKTk5WWlqatW7fq6aef1q9+9atgDwUAACqgoMfHH/7wB40bN04PPPCADh8+rLp162rw4MF65JFHgj0UAACogIIeH3FxcZo+fbqmT58e7IcGAAAXAb7bBQAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE6FJD4+++wz9enTRzVq1FBMTIyaNm2qzZs3h2IoAABQwUQF+wGPHj2qdu3aqVOnTnrrrbd02WWXac+ePapevXqwhwIAABVQ0OPjiSeeUFJSkl544YXAsgYNGgR7GAAAUEEF/W2X1157Tdddd51+9rOfqVatWmrZsqXmzJlzzu39fr8KCgpK3QAAwMUr6PHxySefaNasWWrUqJHeeecd3X///Ro2bJjmzZtX5vYZGRny+XyBW1JSUrCnBAAAypGgx0dJSYmuvfZaTZ48WS1bttSgQYM0cOBAzZ49u8zt09PTlZ+fH7jl5eUFe0oAAKAcCXp81KlTR40bNy61LDU1Vfv27Stze4/Ho/j4+FI3AABw8Qp6fLRr1067d+8utSw7O1v169cP9lAAAKACCnp8jBw5Uhs2bNDkyZOVk5OjBQsW6Pnnn9eQIUOCPRQAAKiAgh4f119/vZYsWaKFCxeqSZMmmjhxoqZPn67evXsHeygAAFABBf1zPiTp9ttv1+233x6KhwYAABUc3+0CAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp0IeH1OmTFFERIRGjBgR6qEAAEAFENL42LRpk/70pz+pWbNmoRwGAABUICGLj+PHj6t3796aM2eOqlevHqphAABABROy+BgyZIi6d++uLl26fOd2fr9fBQUFpW4AAODiFRWKB120aJGysrK0adOm7902IyNDEyZMCMU0AABAORT0Mx95eXkaPny45s+frypVqnzv9unp6crPzw/c8vLygj0lAABQjgT9zMeWLVt0+PBhXXvttYFlxcXFWrVqlZ577jn5/X5FRkYG1nk8Hnk8nmBPAwAAlFNBj4/OnTtrx44dpZb1799f11xzjUaPHl0qPAAAwKUn6PERFxenJk2alFrm9XpVo0aNs5YDAIBLD59wCgAAnArJb7t824oVK1wMAwAAKgDOfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOER8AAMAp4gMAADhFfAAAAKeIDwAA4BTxAQAAnCI+AACAU8QHAABwivgAAABOBT0+MjIydP311ysuLk61atXSnXfeqd27dwd7GAAAUEEFPT5WrlypIUOGaMOGDXr33XdVVFSkW265RYWFhcEeCgAAVEBRwX7At99+u9T9uXPnqlatWtqyZYvat28f7OEAAEAFE/JrPvLz8yVJCQkJoR4KAABUAEE/8/HfSkpKNGLECLVr105NmjQpcxu/3y+/3x+4X1BQEMopAQCAMAvpmY8hQ4boww8/1KJFi865TUZGhnw+X+CWlJQUyikBAIAwC1l8DB06VK+//rqWL1+uevXqnXO79PR05efnB255eXmhmhIAACgHgv62i5npN7/5jZYsWaIVK1aoQYMG37m9x+ORx+MJ9jQAAEA5FfT4GDJkiBYsWKBXX31VcXFxOnjwoCTJ5/MpJiYm2MMBAIAKJuhvu8yaNUv5+fnq2LGj6tSpE7gtXrw42EMBAIAKKCRvuwAAAJwL3+0CAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp0IWHzNnzlRKSoqqVKmiNm3aaOPGjaEaCgAAVCAhiY/Fixdr1KhRGj9+vLKystS8eXN17dpVhw8fDsVwAACgAglJfDz99NMaOHCg+vfvr8aNG2v27NmqWrWq/vKXv4RiOAAAUIFEBfsBT506pS1btig9PT2wrFKlSurSpYvWr19/1vZ+v19+vz9wPz8/X5JUUFAQ7KlJkkr8J0LyuBejYP4ZcNzPH8c9PDju4cFxD49QvMaeeUwz+95tgx4fn3/+uYqLi5WYmFhqeWJionbt2nXW9hkZGZowYcJZy5OSkoI9NVwg3/Rwz+DSxHEPD457eHDcwyOUx/3YsWPy+XzfuU3Q4+NCpaena9SoUYH7JSUl+vLLL1WjRg1FRESEcWZuFBQUKCkpSXl5eYqPjw/3dC4ZHPfw4LiHB8c9PC61425mOnbsmOrWrfu92wY9PmrWrKnIyEgdOnSo1PJDhw6pdu3aZ23v8Xjk8XhKLatWrVqwp1XuxcfHXxJ/Ocsbjnt4cNzDg+MeHpfScf++Mx5nBP2C0+joaLVq1UrLli0LLCspKdGyZct04403Bns4AABQwYTkbZdRo0apX79+uu6669S6dWtNnz5dhYWF6t+/fyiGAwAAFUhI4uPuu+/WkSNH9Mgjj+jgwYNq0aKF3n777bMuQsU3bzuNHz/+rLeeEFoc9/DguIcHxz08OO7nFmHn8zsxAAAAQcJ3uwAAAKeIDwAA4BTxAQAAnCI+HOrYsaNGjBgR7mnge6xYsUIRERH66quvwj0V4Cw8j+BiQHzgonDkyBFFR0ersLBQRUVF8nq92rdvX5nb/v3vf1fbtm0lSevWrdMVV1xRan3btm114MCB8/6wHATHo48+qhYtWoR7GgAcID5wUVi/fr2aN28ur9errKwsJSQkKDk5+ZzbtmvXTpK0evXqwH+fER0drdq1a18SH+8PnI9Tp06Fewq4yBAfIVJYWKi+ffsqNjZWderU0VNPPVVq/dGjR9W3b19Vr15dVatWVbdu3bRnz55S28yZM0dJSUmqWrWq7rrrLj399NOX5EfPn49169YFImLNmjVnBcWFbFvW2y5r1qzRj370I8XExCgpKUnDhg1TYWFh8HekgispKdHUqVPVsGFDeTweJScna9KkSZKk0aNH66qrrlLVqlV1xRVXaNy4cSoqKpIkzZ07VxMmTND27dsVERGhiIgIzZ07N4x7Ur6dPn1aQ4cOlc/nU82aNTVu3LjAN4n6/X799re/1eWXXy6v16s2bdpoxYoVgZ/94osvdM899+jyyy9X1apV1bRpUy1cuLDU43fs2FFDhw7ViBEjVLNmTXXt2tXl7oXVmX0/1/GNiIjQ0qVLS/1MtWrVAn9fP/30U0VERGjRokVq27atqlSpoiZNmmjlypWB7c88x7zxxhtq1qyZqlSpohtuuEEffvihpG9eP+Lj4/Xyyy+XGmfp0qXyer06duxY6A6AK4aQuP/++y05Odnee+89++CDD+z222+3uLg4Gz58uJmZ9ejRw1JTU23VqlW2bds269q1qzVs2NBOnTplZmZr1qyxSpUq2bRp02z37t02c+ZMS0hIMJ/PF76dKmf27t1rPp/PfD6fVa5c2apUqWI+n8+io6PN4/GYz+ez+++/38zM5s+fH9g2IiLCYmNjzefzWaVKlczr9ZrP57P58+ebmdny5ctNkh09etTMzHJycszr9dozzzxj2dnZtnbtWmvZsqXde++94dr1cuvhhx+26tWr29y5cy0nJ8dWr15tc+bMMTOziRMn2tq1ay03N9dee+01S0xMtCeeeMLMzE6cOGEPPvigpaWl2YEDB+zAgQN24sSJcO5KudWhQweLjY214cOH265du+zFF1+0qlWr2vPPP29mZgMGDLC2bdvaqlWrLCcnx6ZNm2Yej8eys7PNzOw///mPTZs2zbZu3Woff/yxzZgxwyIjIy0zM/OsMR566CHbtWuX7dq1Kyz7Gg7fd3wl2ZIlS0r9jM/nsxdeeMHMzHJzc02S1atXz15++WX76KOPbMCAARYXF2eff/65mf3/55jU1FT717/+FXiNSElJCbwGDBw40G677bZS4/To0cP69u0b2gPgCPERAseOHbPo6Gh76aWXAsu++OILi4mJseHDh1t2drZJsrVr1wbWf/755xYTExP4mbvvvtu6d+9e6nF79+5NfPyXoqIiy83Nte3bt1vlypVt+/btlpOTY7GxsbZy5UrLzc21I0eOmNk3fya5ubk2Z84cS0tLs9zcXHv11VetTp06lpuba7m5uXbs2DEzOzs+fv3rX9ugQYNKjb169WqrVKmSff311073uTwrKCgwj8cTiI3vM23aNGvVqlXg/vjx46158+Yhmt3Fo0OHDpaammolJSWBZaNHj7bU1FTbu3evRUZG2meffVbqZzp37mzp6ennfMzu3bvbgw8+WGqMli1bBn/yFcB3HV+z84+PKVOmBNYXFRVZvXr1ArF95jlm0aJFgW3OvEYsXrzYzMwyMzMtMjLS9u/fb2Zmhw4dsqioKFuxYkXQ9zkceNslBD7++GOdOnVKbdq0CSxLSEjQ1VdfLUnauXOnoqKiSq2vUaOGrr76au3cuVOStHv3brVu3brU4377/qUuKipKKSkp2rVrl66//no1a9ZMBw8eVGJiotq3b6+UlBTVrFlTkhQbG6uUlBRlZWWpZ8+eSklJ0Y4dO3TbbbcpJSVFKSkpio2NLXOc7du3a+7cuYqNjQ3cunbtqpKSEuXm5rrc5XJt586d8vv96ty5c5nrFy9erHbt2ql27dqKjY3V2LFjz3lRML7bDTfcUOqapBtvvFF79uzRjh07VFxcrKuuuqrU39eVK1fq448/liQVFxdr4sSJatq0qRISEhQbG6t33nnnrD+LVq1aOd2n8uRcx7e4uPi8H+O/v0g1KipK1113XeD5vaxtzrxGnNmmdevWSktL07x58yRJL774ourXr6/27dv/oH0qb0Ly3S6AC2lpadq7d6+KiopUUlKi2NhYnT59WqdPn1ZsbKzq16+vf//739q3b58aN24sSTp58qSioqL07LPPyu/3q1KlSlq0aJH69Omj2bNnlznO8ePHNXjwYA0bNuysdee6qPVSFBMTc85169evV+/evTVhwgR17dpVPp9PixYtOutaKPxvjh8/rsjISG3ZskWRkZGl1p2J62nTpunZZ5/V9OnT1bRpU3m9Xo0YMeKsi0q9Xq+zeVckERERges/zjhz7VKwDRgwQDNnztSYMWP0wgsvqH///hfNhfDERwhceeWVqly5sjIzMwMvTkePHlV2drY6dOig1NRUnT59WpmZmYFf+fziiy+0e/fuwIvk1VdfrU2bNpV63G/fv9S9+eabKioqUufOnTV16lS1atVKvXr10r333qtbb71VlStXliTVrVtX27Zt08GDB9WlSxdt27ZNxcXFatGihVavXq2EhATFx8efc5xrr71WH330kRo2bOhq1yqkRo0aKSYmRsuWLdOAAQNKrVu3bp3q16+v3//+94Fle/fuLbVNdHT0Bf2f5aUsMzOz1P0NGzaoUaNGatmypYqLi3X48GH96Ec/KvNn165dq549e6pPnz6SvrlIODs7O/Dcg3Mf38jISF122WU6cOBAYN2ePXt04sSJsx5jw4YNgbMUp0+f1pYtWzR06NCztvn2a0RqampgfZ8+ffTwww9rxowZ+uijj9SvX7+g7WPYhft9n4vVfffdZ/Xr17dly5bZjh07rEePHoGLmMzMevbsaY0bN7bVq1fbtm3b7NZbby3zgtOnnnrKsrOzbfbs2VajRg2rVq1aGPeq/Dlw4IB5PB77+uuv7eTJk1alSpXAe6TftnDhQmvfvr2Zma1cudKuuuqqMrf79jUf27dvt5iYGBsyZIht3brVsrOzbenSpTZkyJCQ7FNF9uijj1r16tVt3rx5lpOTY+vXr7c///nP9uqrr1pUVJQtXLjQcnJy7Nlnnz3rAur58+eb1+u1rVu32pEjR+zkyZPh25Fy7MwFkSNHjrRdu3bZggULzOv12uzZs83sm2vDUlJS7B//+Id98sknlpmZaZMnT7bXX3/dzMxGjhxpSUlJtnbt2sDFkPHx8dazZ89SY5x5rrrUfN/x7dWrl6WmplpWVpZt2rTJfvzjH1vlypXPuuYjOTnZXnnlFdu5c6cNGjTIYmNjA9egnXmOSUtLs/feey/wGpGcnGx+v7/UfH7xi19YdHS03XrrrU6PQ6gRHyFy7Ngx69Onj1WtWtUSExNt6tSppf5Bf/nll/bLX/7SfD6fxcTEWNeuXQNXo5/x/PPP2+WXX24xMTF255132uOPP261a9cOw96UXwsXLrSbbrrJzMxWrVplDRs2POe2gwcPtrFjx5qZ2WOPPWYDBgwoc7tvx4eZ2caNG+3mm2+22NhY83q91qxZM5s0aVLwduQiUVxcbI8//rjVr1/fKleubMnJyTZ58mQzM3vooYesRo0aFhsba3fffbc988wzpeLj5MmT9pOf/MSqVatmkgJP5iitQ4cO9sADD9h9991n8fHxVr16dfvd734XuEDy1KlT9sgjj1hKSopVrlzZ6tSpY3fddZd98MEHZvbNhY09e/a02NhYq1Wrlo0dO9b69u1LfPyf7zu+n332md1yyy3m9XqtUaNG9uabb5Z5wemCBQusdevWFh0dbY0bN7b3338/MMaZ55h//vOflpaWZtHR0da6dWvbvn37WfNZtmyZSSr1CwwXgwizb715hXJr4MCB2rVrl1avXh3uqQDARaljx45q0aKFpk+f/oN+/tNPP1WDBg20devWc35i74oVK9SpUycdPXr0ez+76W9/+5tGjhyp/fv3Kzo6+gfNqTzimo9y7Mknn9TNN98sr9ert956S/PmzdMf//jHcE8LABBiJ06c0IEDBzRlyhQNHjz4ogoPiU84Ldc2btyom2++WU2bNtXs2bM1Y8aMsy7kAwBcfKZOnaprrrlGtWvXVnp6erinE3S87QIAAJzizAcAAHCK+AAAAE4RHwAAwCniAwAAOEV8AAAAp4gPAADgFPEBAACcIj4AAIBTxAcAAHDq/wHa+vZEbX05kAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(words, top.values.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "792etHKPSZrx",
        "tags": []
      },
      "source": [
        "Według modelu najbardziej prawdopodobnym uzupełnieniem brakującego wyrazu jest `dog` (a nie `fox`). Nieco zaskakujący może być drugi wyraz `##ie`, ale po dodaniu go do istniejącego tekstu otrzymamy zdanie: \"The quick brownie jumps over the lazy dog\", które również wydaje się sensowne (choć nieco zaskakujące)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "wHY-BZsXsEmH"
      },
      "source": [
        "## Zadanie 2 (1.5 punkty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "1QK7MybnTT-h",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "Wykorzystując model `allegro/herbert-base-cased` zaproponuj zdania z jednym brakującym wyrazem, weryfikujące zdolność tego modelu do:\n",
        "* odmiany przez polskie przypadki,\n",
        "* uwzględniania długodystansowych związków w tekście,\n",
        "* reprezentowania wiedzy o świecie.\n",
        "\n",
        "Dla każdego problemu wymyśl po 3 zdania sprawdzające i wyświetl predykcję dla 5 najbardziej prawdopodobnych wyrazów.\n",
        "\n",
        "Możesz wykorzystać kod z funkcji `plot_words`, który ułatwi Ci wyświetlanie wyników. Zweryfikuj również jaki token maskujący wykorzystywany jest w tym modelu. Pamiętaj również o załadowaniu modelu `allegro/herbert-base-cased`.\n",
        "\n",
        "Oceń zdolności modelu w zakresie wskazanych zadań."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-12-20T13:58:13.903939Z",
          "start_time": "2022-12-20T13:58:13.886635Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "iy1RYqMvTKEe",
        "outputId": "139f4380-e7d5-43a3-ba4c-e7cab0ef250f",
        "tags": [
          "ex"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask token for HerbertTokenizerFast(name_or_path='allegro/herbert-base-cased', vocab_size=50000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "): <mask>\n"
          ]
        }
      ],
      "source": [
        "def plot_words(sentence, word_model, word_tokenizer, mask=\"[MASK]\"):\n",
        "    sentence = word_tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "    tokens = word_tokenizer.convert_ids_to_tokens(list(sentence[0]))\n",
        "    print(\"|\".join(tokens))\n",
        "    print(device)\n",
        "    target = word_model(sentence.to(device))\n",
        "    top = torch.topk(target.logits[0][tokens.index(mask)], 5)\n",
        "    words = word_tokenizer.convert_ids_to_tokens(top.indices)\n",
        "    mask_token = word_tokenizer.encode(mask, add_special_tokens=False)[0]\n",
        "    token_ids = list(sentence[0].cpu().detach().numpy())\n",
        "    mask_index = token_ids.index(mask_token)\n",
        "    for word_id in top.indices:\n",
        "        token_ids[mask_index]  = word_id\n",
        "        print(word_tokenizer.decode(token_ids, skip_special_tokens=True))\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.bar(words, top.values.cpu().detach().numpy())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# your_code\n",
        "\n",
        "model_pl = AutoModelForMaskedLM.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "tokenizer_pl =  AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "\n",
        "mask_token = tokenizer2.mask_token\n",
        "print(f\"Mask token for {tokenizer_pl}: {mask_token}\")\n",
        "\n",
        "sentence_cases_pl_text_1 = \"Kacper zawołał swoją <mask>\"\n",
        "sentence_cases_pl_text_2 = \"Marcin skacze na <mask>\"\n",
        "sentence_cases_pl_text_3 = \"Kuba wyszedł na spacer z <mask>\"\n",
        "sentence_relations_pl_text_1 = \"Szymon miał kapelusz w kolorze zielonym z dużym rantem, który <mask> w każdy piątek\"\n",
        "sentence_relations_pl_text_2 = \"\"\n",
        "sentence_relations_pl_text_3 = \"\"\n",
        "sentence_world_knowledge_pl_text_1 = \"Ziemia podzielona jest na 7 <mask>\"\n",
        "sentence_world_knowledge_pl_text_2 = \"Pierwszy prezydent Trzeciej Rzeczpospolitej miał na nazwisko <mask>\"\n",
        "sentence_world_knowledge_pl_text_3 = \"Tadż Mahal został zbudowant na cześć zmarłej <mask>\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_words(sentence_cases_pl_text_1,model_pl,tokenizer_pl,\"<mask>\")\n",
        "plot_words(sentence_cases_pl_text_2,model_pl,tokenizer_pl,\"<mask>\")\n",
        "plot_words(sentence_cases_pl_text_3,model_pl,tokenizer_pl,\"<mask>\")\n",
        "plot_words(sentence_relations_pl_text_1,model_pl,tokenizer_pl,\"<mask>\")\n",
        "plot_words(sentence_world_knowledge_pl_text_1,model_pl,tokenizer_pl,\"<mask>\")\n",
        "plot_words(sentence_world_knowledge_pl_text_2,model_pl,tokenizer_pl,\"<mask>\")\n",
        "plot_words(sentence_world_knowledge_pl_text_3,model_pl,tokenizer_pl,\"<mask>\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m92XuZ81xn8P",
        "outputId": "36f4f33f-2836-4d3f-8ff7-2719e08baf58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>|Kacper</w>|zawołał</w>|swoją</w>|<mask>|</s>\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-18cfde038204>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_cases_pl_text_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<mask>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_cases_pl_text_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<mask>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_cases_pl_text_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<mask>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_relations_pl_text_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<mask>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_world_knowledge_pl_text_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_pl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<mask>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-8f1f376b7049>\u001b[0m in \u001b[0;36mplot_words\u001b[0;34m(sentence, word_model, word_tokenizer, mask)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "C0D3wjqU5E7s",
        "tags": [
          "ex"
        ]
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "qe3jkYN4X0K6",
        "tags": []
      },
      "source": [
        "# Klasyfikacja tekstu\n",
        "\n",
        "Pierwszym zadaniem, które zrealizujemy korzystając z modelu HerBERT będzie klasyfikacja tekstu. Będzie to jednak dość nietypowe zadanie. O ile oczekiwanym wynikiem jest klasyfikacja binarna, czyli dość popularny typ klasyfikacji, o tyle dane wejściowe są nietypowe, gdyż są to pary: `(pytanie, kontekst)`. Celem algorytmu jest określenie, czy na zadane pytanie można odpowiedzieć na podstawie informacji znajdujących się w kontekście.\n",
        "\n",
        "Model tego rodzaju jest nietypowy, ponieważ jest to zadanie z zakresu klasyfikacji par tekstów, ale my potraktujemy je jak zadanie klasyfikacji jednego tekstu, oznaczając jedynie fragmenty tekstu jako `Pytanie:` oraz `Kontekst:`. Wykorzystamy tutaj zdolność modeli transformacyjnych do automatycznego nauczenia się tego rodzaju znaczników, przez co proces przygotowania danych będzie bardzo uproszczony.\n",
        "\n",
        "Zbiorem danych, który wykorzystamy do treningu i ewaluacji modelu będzie PoQUAD - zbiór inspirowany angielskim [SQuADem](https://rajpurkar.github.io/SQuAD-explorer/), czyli zbiorem zawierającym ponad 100 tys. pytań i odpowiadających im odpowiedzi. Zbiór ten powstał niedawno i jest jeszcze rozbudowywany. Zawiera on pytania, odpowiedzi oraz konteksty, na podstawie których można udzielić odpowiedzi.\n",
        "\n",
        "W dalszej części laboratorium skoncentrujemy się na problemie odpowiadania na pytania."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "xJFq2RGgVArz",
        "tags": []
      },
      "source": [
        "## Przygotowanie danych do klasyfikacji\n",
        "\n",
        "Przygotowanie danych rozpoczniemy od sklonowania repozytorium zawierającego pytania i odpowiedzi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "ASJlTuYmxnsO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"clarin-pl/poquad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "1IArBUss6j5L",
        "tags": []
      },
      "source": [
        "Sprawdźmy co znajduje się w zbiorze danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "MpE1sTIuwKr0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "Qu_APsiB6mLo",
        "tags": []
      },
      "source": [
        "Zbiór danych jest podzielony na dwie części: treningową i walidacyjną. Rozmiar części treningowej to ponad 46 tysięcy pytań i odpowiedzi, natomiast części walidacyjnej to ponad 5 tysięcy pytań i odpowiedzi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "GxdjcmsD6yc6",
        "tags": []
      },
      "source": [
        "Dane zbioru przechowywane są w plikach `poquad_train.json` oraz `poquad_dev.json`. Dostarczenie podziału na te grupy danych jest bardzo częstą praktyką w przypadku publicznych, dużych zbiorów danych, gdyż umożliwia porównywanie różnych modeli, korzystając z dokładnie takiego samego zestawu danych. Prawdopodobnie istnieje również zbiór `poquad_test.json`, który jednak nie jest udostępniany publicznie. Tak jest w przypadku SQuADu - twórcy zbioru automatycznie ewaluują dostarczane modele, ale nie udstoępniaja zbioru testowego. Dzięki temu trudniej jest nadmiernie dopasować model do danych testowych.\n",
        "\n",
        "Struktura każdej z dostępnych części jest taka sama. Zgodnie z powyższą informacją zawiera ona następujące elementy:\n",
        "* `id` - identyfikator pary: pytanie - odpowiedź,\n",
        "* `title` - tytuł artykułu z Wikipedii, na podstawie którego utworzono parę,\n",
        "* `context` - fragment treści artykułu z Wikipedii, zawierający odpowiedź na pytanie,\n",
        "* `question` - pytanie,\n",
        "* `answers` - odpowiedzi.\n",
        "\n",
        "Możemy wyświetlić kilka począkotwych wpisów części treningowej:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "i3ZLmxlzx4wd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset['train']['question'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "-YewsI8Dymaq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset['train']['answers'][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "_rlhCQp_3kEJ",
        "tags": []
      },
      "source": [
        "Niestety, autorzy zbioru danych, pomimo tego, że dane te znajdują się w źródłowym zbiorze danych, nie udostępniają dwóch ważnych informacji: o tym, czy można odpowiedzieć na dane pytanie oraz jak brzmi generatywna odpowiedź na pytanie. Dlatego póki nie zostanie to naprawione, będziemy dalej pracowąć z oryginalnymi plikami zbioru danych, które dostępne są na stronie opisującej zbiór danych: https://huggingface.co/datasets/clarin-pl/poquad/tree/main\n",
        "\n",
        "Pobierz manualnie zbiory `poquad-dev.json` oraz `poquad-train.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "JoRrYJfO4Gs1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/datasets/clarin-pl/poquad/raw/main/poquad-dev.json\n",
        "!wget https://huggingface.co/datasets/clarin-pl/poquad/resolve/main/poquad-train.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "NPQoBTFn4S30",
        "tags": []
      },
      "source": [
        "Dla bezpieczeństwa, jeśli korzystamy z Google drive, to przeniesiemy pliki do naszego dysku:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "OtqQsRgB4O-W",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!mkdir gdrive/MyDrive/poquad\n",
        "!mv poquad-dev.json gdrive/MyDrive/poquad\n",
        "!mv poquad-train.json gdrive/MyDrive/poquad\n",
        "\n",
        "!head -30 gdrive/MyDrive/poquad/poquad-dev.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "QjFnqM538V_9",
        "tags": []
      },
      "source": [
        "Struktura pliku odpowiada strukturze danych w zbiorze SQuAD. Dane umieszczone są w kluczu `data` i podzielone na krotki odpowiadające pojedynczym artykułom Wikipedii. W ramach artykułu może być wybranych jeden lub więcej paragrafów, dla których w kluczu `qas` pojawiają się pytania (`question`), flaga `is_impossible`, wskazujace czy można odpowiedzieć na pytanie oraz odpowiedzi (o ile nie jest ustawiona flaga `is_impossible`). Odpowiedzi może być wiele i składają się one z treści odpowiedzi (`text`) traktowanej jako fragment kontekstu, a także naturalnej odpowiedzi na pytanie (`generative_answer`).\n",
        "\n",
        "Taki podział może wydawać się dziwny, ale zbiór SQuAD zawiera tylko odpowiedzi pierwszego rodzaju. Wynika to z faktu, że w języku angielskim fragment tekstu będzie często stanowił dobrą odpowiedź na pytanie (oczywiście z wyjątkiem pytań dla których odpowiedź to `tak` lub `nie`).\n",
        "\n",
        "Natomiast ten drugi typ odpowiedzi jest szczególnie przydatny dla języka polskiego, ponieważ często odpowiedź chcemy syntaktycznie dostosować do pytania, co jest niemożliwe, jeśli odpowiedź wskazywana jest jako fragment kontekstu.\n",
        "W sytuacji, w której odpowiedzi były określane w sposób automatyczny, są one oznaczone jako `plausible_answers`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "SFFtpJOLsEmU"
      },
      "source": [
        "## Ładowanie danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "YRYW41hcsEmU"
      },
      "source": [
        "Zaczniemy od wczytania danych i wyświetlenia podstawowych statystyk dotyczących ilości artykułów oraz przypisanych do nich pytań."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "BDbf_9LKxuyJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Adjust for your needs\n",
        "path = \".\"\n",
        "#path = 'gdrive/MyDrive/poquad'\n",
        "\n",
        "with open(path + \"/poquad-train.json\") as input:\n",
        "    train_data = json.loads(input.read())[\"data\"]\n",
        "\n",
        "print(f\"Train data articles: {len(train_data)}\")\n",
        "\n",
        "with open(path +\"/poquad-dev.json\") as input:\n",
        "    dev_data = json.loads(input.read())[\"data\"]\n",
        "\n",
        "print(f\"Dev data articles: {len(dev_data)}\")\n",
        "\n",
        "print(f\"Train questions: {sum([len(e['paragraphs'][0]['qas']) for e in train_data])}\")\n",
        "print(f\"Dev questions: {sum([len(e['paragraphs'][0]['qas']) for e in dev_data])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "vrLTRuCz-4nv",
        "tags": []
      },
      "source": [
        "Ponieważ w pierwszym problemie chcemy stwierdzić, czy na pytanie można udzielić odpowiedzi na podstawie kontekstu, połączymy wszystkie konteksty w jedną tablicę, aby móc losować z niej dane negatywne, gdyż liczba pytań nie posiadających odpowiedzi jest stosunkowo mała, co prowadziłoby utworzenia niezbalansowanego zbioru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "c-1WgbVA1wsy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "all_contexts = [e[\"paragraphs\"][0][\"context\"] for e in train_data] + [\n",
        "    e[\"paragraphs\"][0][\"context\"] for e in dev_data\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "3Md-nxc7_jPy",
        "tags": []
      },
      "source": [
        "W kolejnym kroku zamieniamy dane w formacie JSON na reprezentację zgodną z przyjętym założeniem.\n",
        "Chcemy by kontekst oraz pytanie występowały obok siebie i każdy z elementów był sygnalizowany wyrażeniem: `Pytanie:` i `Kontekst:`. Treść klasyfikowanego tekstu przyporządkowujemy do klucza `text`, natomiast klasę do klucza `label`, gdyż takie są oczekiwanie biblioteki Transformer.\n",
        "\n",
        "Pytania, które mają ustawiną flagę `is_impossible` na `True` trafiają wprost do przekształconego zbioru. Dla pytań, które posiadają odpowiedź, dodatkowo losowany jest jeden kontekst, który stanowi negatywny przykład. Weryfikujemy tylko, czy kontekst ten nie pokrywa się z kontekstem, który przypisany był do pytania. Nie przeprowadzamy bardziej zaawansowanych analiz, które pomogłyby wylkuczyć sytuację, w której inny kontekst również zawiera odpowiedź na pytanie, gdyż prawdopodobieństwo wylosowania takiego kontekstu jest bardzo małe.\n",
        "\n",
        "Na końcu wyświetlamy statystyki utworzonego zbioru danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "lbCkeE_f5Yg8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "tuples = [[], []]\n",
        "\n",
        "for idx, dataset in enumerate([train_data, dev_data]):\n",
        "    for data in dataset:\n",
        "        context = data[\"paragraphs\"][0][\"context\"]\n",
        "        for question_answers in data[\"paragraphs\"][0][\"qas\"]:\n",
        "            question = question_answers[\"question\"]\n",
        "            if question_answers[\"is_impossible\"]:\n",
        "                tuples[idx].append(\n",
        "                    {\n",
        "                        \"text\": f\"Pytanie: {question} Kontekst: {context}\",\n",
        "                        \"label\": 0,\n",
        "                    }\n",
        "                )\n",
        "            else:\n",
        "                tuples[idx].append(\n",
        "                    {\n",
        "                        \"text\": f\"Pytanie: {question} Kontekst: {context}\",\n",
        "                        \"label\": 1,\n",
        "                    }\n",
        "                )\n",
        "                while True:\n",
        "                    negative_context = random.choice(all_contexts)\n",
        "                    if negative_context != context:\n",
        "                        tuples[idx].append(\n",
        "                            {\n",
        "                                \"text\": f\"Pytanie: {question} Kontekst: {negative_context}\",\n",
        "                                \"label\": 0,\n",
        "                            }\n",
        "                        )\n",
        "                        break\n",
        "\n",
        "train_tuples, dev_tuples = tuples\n",
        "print(f\"Total count in train/dev: {len(train_tuples)}/{len(dev_tuples)}\")\n",
        "print(\n",
        "    f\"Positive count in train/dev: {sum([e['label'] for e in train_tuples])}/{sum([e['label'] for e in dev_tuples])}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "V2fQbatcAj5b",
        "tags": []
      },
      "source": [
        "Widzimy, że uzyskane zbiory danych cechują się dość dobrym zbalansowaniem.\n",
        "\n",
        "Dobrą praktyką po wprowadzeniu zmian w zbiorze danych, jest wyświetlenie kilku przykładowych punktów danych, w celu wykrycia ewentualnych błędów, które powstały na etapie konwersji zbioru. Pozwala to uniknąć nieprzyjemnych niespodzianek, np. stworzenie identycznego zbioru danych testowych i treningowych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "Lr-oeLgR9H75",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(train_tuples[0:1])\n",
        "print(dev_tuples[0:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "bTTry7LfBXKb",
        "tags": []
      },
      "source": [
        "Ponieważ mamy nowe zbiory danych, możemy opakować je w klasy ułatwiające manipulowanie nimi. Ma to szczególne znaczenie w kontekście szybkiej tokenizacji tych danych, czy późniejszego szybkiego wczytywania wcześniej utworzonych zbiorów danych.\n",
        "\n",
        "W tym celu wykorzystamy bibliotekę `datasets`. Jej kluczowymi klasami są `Dataset` reprezentujący jeden z podzbiorów zbioru danych (np. podzbiór testowy) oraz `DatasetDict`, który łączy wszystkie podzbiory w jeden obiekt, którym możemy manipulować w całości. (Gdyby autorzy udostępnili odpowiedni skrypt ze zbiorem, moglibyśmy wykorzystać tę bibliotekę bez dodatkowej pracy).\n",
        "\n",
        "Dodatkowo zapiszemy tak utworzony zbiór danych na dysku. Jeśli później chcielibyśmy wykorzystać stworzony zbiór danych, to możemy to zrobić za pomocą komendy `load_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "rtTsPgmiDdG8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_list(train_tuples)\n",
        "dev_dataset = Dataset.from_list(dev_tuples)\n",
        "datasets = DatasetDict({\"train\": train_dataset, \"dev\": dev_dataset})\n",
        "datasets.save_to_disk(path + \"/question-context-classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ORcWOWjiCAhu",
        "tags": []
      },
      "source": [
        "Dane tekstowe przed przekazaniem do modelu wymagają tokenizacji (co widzieliśmy już wcześniej). Efektywne wykonanie tokenizacji na całym zbiorze danych ułatwione jest przez obiekt `DatasetDict`. Definiujemy funkcję `tokenize_function`, która korzystając z załadowanego tokenizera, zamienia tekst na identyfikatory.\n",
        "\n",
        "W wywołaniu używamy opcji `padding` - uzupełniamy wszystkie teksty do długości najdłuższego tekstu. Dodatkowo, jeśli któryś tekst wykracza poza maksymalną długość obsługiwaną przez model, to jest on przycinany (`truncation=True`).\n",
        "\n",
        "Tokenizację aplikujemy do zbioru z wykorzystaniem przetwarzania batchowego (`batched=True`), które pozwala na szybsze stokenizowanie dużego zbioru danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "WLJSYvpFFlfO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "pl_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return pl_tokenizer(examples[\"text\"], padding='do_not_pad', truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "X5FJ54OLS0hK",
        "tags": []
      },
      "source": [
        "Stokenizowane dane zawierają dodatkowe pola: `input_ids`, `token_type_ids` oraz `attention_mask`. Dla nas najważniejsze jest pole `input_ids`, które zawiera identyfikatory tokenów. Pozostałe dwa pola są ustawione na identyczne wartości (wszystkie tokeny mają ten sam typ, maska atencji zawiera same jedynki), więc nie są one dla nas zbyt interesujące. Zobaczmy pola `text`, `input_ids` oraz `attention_mask` dla pierwszego przykładu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "mgCExFTHSEYq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "example = tokenized_datasets[\"train\"][0]\n",
        "print(example[\"text\"])\n",
        "print(\"-\" * 60)\n",
        "print(example[\"input_ids\"])\n",
        "print(\"-\" * 60)\n",
        "print(example[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "WnG0EEHi782A",
        "tags": []
      },
      "source": [
        "Możem też sprawdzić, jak został stokenizowany pierwszy przykład:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "NsfJDuhN8Acj",
        "lines_to_next_cell": 2,
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"|\".join(pl_tokenizer.convert_ids_to_tokens(list(example[\"input_ids\"]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "-DL-RiReUT6e",
        "tags": []
      },
      "source": [
        "Widzimy, że wyrazy podzielone są sensownie.\n",
        "\n",
        "Możemy sprawdzić, że liczba tokenów w polu `inut_ids`, które są różne od tokenu wypełnienia (`[PAD] = 1`) oraz maska atencji, mają tę samą długość:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "QeSZdD09T7TH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(len([e for e in example[\"input_ids\"] if e != 1]))\n",
        "print(len([e for e in example[\"attention_mask\"] if e == 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "KKm4X7jzUjW7",
        "tags": []
      },
      "source": [
        "Mając pewność, że przygotowane przez nas dane są prawidłowe, możemy przystąpić do procesu uczenia modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "OmVeK74JVPKz",
        "tags": []
      },
      "source": [
        "## Trening z użyciem transformersów\n",
        "\n",
        "Biblioteka Transformes pozwala na załadowanie tego samego modelu dostosowanego do różnych zadań. Wcześniej używaliśmy modelu HerBERT do predykcji brakującego wyrazu. Teraz załadujemy ten sam model, ale z inną \"głową\". Zostanie użyta warstwa, która pozwala na klasyfikację całego tekstu do jednej z n-klas. Wystarczy podmienić klasę, za pomocą której ładujemy model na `AutoModelForSequenceClassification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "cVs4tK1WHUT8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"allegro/herbert-base-cased\", num_labels=2\n",
        ")\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "DON52QQtsEmq"
      },
      "source": [
        "Aby przyspieszyć trening, będziemy chcieli wybrać tylko niektóre spośród wszystkich trenowalnych parametrów. Wyświetlmy zatem listę nazw dostępnych parametrów, w tym modelu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "vEGlX59QsEmr"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "uei_oQ3_sEmr"
      },
      "source": [
        "## Zadanie 3 (0.5 punktu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "tXJl5FnUsEmr"
      },
      "source": [
        "Korzystając z atrybutu `requires_grad`, spraw aby następujące parametry:\n",
        "* klasyfikatory,\n",
        "* warstwy poolingu,\n",
        "* ostatniej warstwy encodera,\n",
        "\n",
        "były jedynymi parametrami podlegającymi uczeniu. Zwróć uwagę na fakt, że domyślnie wszystkie parametry podlegają uczeniu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "psw_tHRWsEms"
      },
      "outputs": [],
      "source": [
        "# your_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "RonUdFRrsEmx"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    trainable = 100 * trainable_params / all_params\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} \"\n",
        "        f\"|| trainable%: {trainable:.4f}%\"\n",
        "    )\n",
        "    return trainable\n",
        "\n",
        "trainable_proportion = print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "j206BCZCsEmy"
      },
      "outputs": [],
      "source": [
        "assert 5 < trainable_proportion < 7\n",
        "print(\"Solution correct!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "axdrBfSuE5YO",
        "tags": []
      },
      "source": [
        "Komunikat diagnostyczny, który pojawia się przy ładowaniu modelu jest zgodny z naszymi oczekiwaniami. Model HerBERT był trenowany do predykcji tokenów, a nie klasyfikacji tekstu. Dlatego też ostatnia warstwa (`classifier.weight` oraz `classifier.bias`) jest inicjowana losowo. Wagi zostaną ustalone w trakcie procesu fine-tuningu modelu.\n",
        "\n",
        "Jeśli porównamy wersje modeli załadowane za pomocą różnych klas, to zauważymy, że różnią się one tylko na samym końcu. Jest to zgodne z założeniami procesu pre-treningu i fine-tuningu. W pierwszy etapie model uczy się zależności w języku, korzystając z zadania maskowanego modelowania języka (Masked Language Modeling). W drugim etapie model dostosowywane jest do konkretnego zadania, np. klasyfikacji binarnej tekstu.\n",
        "\n",
        "Korzystanie z biblioteki Transformers uwalnia nas od manualnego definiowania pętli uczącej, czy wywoływania algorytmu wstecznej propagacji błędu. Trening realizowany jest z wykorzystaniem klasy `Trainer`  (i jej specjlizacji). Argumenty treningu określane są natomiast w klasie `TrainingArguments`.  Klasy te są [bardzo dobrze udokumentowane](https://huggingface.co/docs/transformers/main_classes/trainer#trainer), więc nie będziemy omawiać wszystkich możliwych opcji.\n",
        "\n",
        "Najważniejsze opcje są następujące:\n",
        "* `output_dir` - katalog do którego zapisujemy wyniki,\n",
        "* `do_train` - wymagamy aby przeprowadzony był trening,\n",
        "* `do_eval` - wymagamy aby przeprowadzona była ewaluacja modelu,\n",
        "* `evaluation_strategy` - określenie momentu, w którym realizowana jest ewaluacja,\n",
        "* `evaluation_steps` - określenie co ile kroków (krok = przetworzenie 1 batcha) ma być realizowana ewaluacja,\n",
        "* `per_device_train/evaluation_batch_size` - rozmiar batcha w trakcie treningu/ewaluacji,\n",
        "* `learning_rate` - szybkość uczenia,\n",
        "* `num_train_epochs` - liczba epok uczenia,\n",
        "* `logging`... - parametry logowania postępów uczenia,\n",
        "* `save_strategy` - jak często należy zapisywać wytrenowany model,\n",
        "* `fp16/bf16` - użycie arytmetyki o zmniejszonej dokładności, przyspieszającej proces uczenia. **UWAGA**: użycie niekompatybilnej arytmetyki skutkuje niemożnością nauczenia modelu, co jednak nie daje żadnych innych błędów lub komunikatów ostrzegawczych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "Iub6XtjPH7O6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=path + \"/output\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=256,\n",
        "    learning_rate=2e-04,\n",
        "    num_train_epochs=1,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    fp16=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    seed=42,\n",
        "    load_best_model_at_end=True,\n",
        "    label_smoothing_factor=0.1,\n",
        "    group_by_length=True,\n",
        "    eval_on_start=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "vlShURnsVAXC",
        "tags": []
      },
      "source": [
        "W trakcie treningu będziemy chcieli zobaczyć, czy model poprawnie radzi sobie z postawionym mu problemem. Najlepszym sposobem na podglądanie tego procesu jest obserwowanie wykresów. Model może raportować szereg metryk, ale najważniejsze dla nas będą następujące wartości:\n",
        "* wartość funkcji straty na danych treningowych - jeślie nie spada w trakcie uczenia, znaczy to, że nasz model nie jest poprawnie skonstruowany lub dane uczące są niepoprawne,\n",
        "* wartość jednej lub wielu metryk uzyskiwanych na zbiorze walidacyjnym - możemy śledzić wartość funkcji straty na zbiorze ewaluacyjnym, ale warto również wyświetlać metryki, które da się łatwiej zinterpretować; dla klasyfikacji zbalansowanego zbioru danych może to być dokładność (`accuracy`).\n",
        "\n",
        "Biblioteka Transformers pozwala w zasadzie na wykorzystanie dowolnej metryki, ale szczególnie dobrze współpracuje z metrykami zdefiniowanymi w bibliotece `evaluate` (również autorstwa Huggingface).\n",
        "\n",
        "Wykorzystanie metryki wymaga od nas zdefiniowania metody, która akceptuje batch danych, który zawieraja predykcje (wektory zwrócone na wyjściu modelu) oraz referencyjne wartości - wartości przechowywane w kluczu `label`. Przed obliczeniem metryki konieczne jest \"odcyfrowanie\" zwróconych wartości. W przypadku klasyfikacji oznacza to po prostu wybranie najbardziej prawodopodobnej klasy i porównanie jej z klasą referencyjną.\n",
        "\n",
        "Użycie konkretnej metryki realizowane jest za pomocą wywołania `metric.compute`, która akceptuje predykcje (`predictions`) oraz wartości referencyjne (`references`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "S861cZksGrWM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "f1qk791L6_I7",
        "tags": []
      },
      "source": [
        "Ostatnim krokiem w procesie treningu jest stworzenie obiektu klasy `Trainer`. Akceptuje ona m.in. model, który wykorzystywany jest w treningu, przygotowane argumenty treningu, zbiory do treningu, ewaluacji, czy testowania oraz wcześniej określoną metodę do obliczania metryki na danych ewaluacyjnych.\n",
        "\n",
        "W przetwarzaniu jezyka naturalnego dominującym podejściem jest obecnie rozdzielenie procesu treningu na dwa etapy: pre-treining oraz fine-tuning. W pierwszym etapie model trenowany jest w reżimie self-supervised learning (SSL). Wybierane jest zadanie związane najczęściej z modelowaniem języka - może to być kauzalne lub maskowane modelowanie języka.\n",
        "\n",
        "W *kauzalnym modelowaniu języka* model językowy, na podstawie poprzedzających wyrazów określa prawdopodobieństwo wystąpienia kolejnego wyrazu. W *maskowanym modelowaniu języka* model językowy odgaduje w tekście część wyrazów, która została z niego usunięta.\n",
        "\n",
        "W obu przypadkach dane, na których trenowany jest model nie wymagają ręcznego oznakowania (tagowaina). Wystarczy jedynie posiadać duży korpus danych językowych, aby wytrenować model, który dobrze radzi sobie z jednym z tych zadań. Model tego rodzaju był pokazany na początku laboratorium.\n",
        "\n",
        "W drugim etapie - fine-tuningu (dostrajaniu modelu) - następuje modyfikacja parametrów modelu, w celu rozwiązania konkretnego zadania. W naszym przypadku pierwszym zadaniem tego rodzaju jest klasyfikacja. Dostroimy zatem model `herbert-base-cased` do zadania klasyfikacji par: pytanie - kontekst.\n",
        "\n",
        "Wykorzystamy wcześniej utworzone zbiory danych i dodatkowo zmienimy kolejność danych, tak aby uniknąć potencjalnego problemu z korelacją danych w ramach batcha. Wykorzystujemy do tego wywołanie `shuffle`. Za pomocą funkcji `select` możemy wybrać podzbiór przykładów. Jeśli trening trwa u nas wyjątkowo długo, możemy zmienić\n",
        "domyślne wartości na mniejsze.\n",
        "\n",
        "Ostatnim elementem jest tzw. `data collator`. Dzięki niemu wszystkie przykłady w jednym batchu mają taką samą długość i mogą być przekształcone do\n",
        "tensora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "zSM6Qmv_WUgz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "seed = 42\n",
        "train_examples_count = len(tokenized_datasets[\"train\"])\n",
        "print(train_examples_count)\n",
        "dev_examples_count = len(tokenized_datasets[\"dev\"])\n",
        "print(dev_examples_count)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=tokenized_datasets[\"train\"].select(range(train_examples_count)).shuffle(seed=seed),\n",
        "    eval_dataset=tokenized_datasets[\"dev\"].select(range(dev_examples_count)).shuffle(seed=seed),\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=pl_tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "kx8WSdqx9Hv5",
        "tags": []
      },
      "source": [
        "Zanim uruchomimy trening, załadujemy jeszcze moduł TensorBoard. Nie jest to krok niezbędy. TensorBoard to biblioteka, która pozwala na wyświetlanie w trakcie procesu trening wartości, które wskazują nam, czy model trenuje się poprawnie. W naszym przypadku będzie to `loss` na danych treningowych, `loss` na danych ewaluacyjnych oraz wartość metryki `accuracy`, którą zdefiniowaliśmy wcześniej. Wywołanie tej komórki na początku nie da żadnego efektu, ale można ją odświeżać, za pomocą ikony w menu TensorBoard (ewentualnie włączyć automatyczne odświeżanie). Wtedy w miarę upływu treningu będziemy mieli podgląd, na przebieg procesu oraz osiągane wartości interesujących nas parametrów.\n",
        "\n",
        "Warto zauważyć, że istenieje szereg innych narzędzi do monitorowania eksperymentów z treningiem sieci. Wśród nich dużą popularnością cieszą się [WanDB](https://wandb.ai/site) oraz [Neptune.AI](https://neptune.ai/). Ich zaletą jest m.in. to, że możemy łatwo archiwizować przeprowadzone eksperymenty, porównywać je ze sobą, analizować wpływ hiperparametrów na uzyskane wyniki, itp.\n",
        "\n",
        "Jeśli wyniki są niewidoczne, otwórz ręcznie adres, np. http://localhost:6006 jeśli uruchamiasz notebooka lokalnie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "Qg3S3CanFoBE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./output/runs\n",
        "%load_ext tensorboard\n",
        "#%tensorboard --logdir gdrive/MyDrive/poquad/output/runs\n",
        "%tensorboard --logdir ./output/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "W5d5E2OO-P5C",
        "tags": []
      },
      "source": [
        "Uruchomienie procesu treningu jest już bardzo proste, po tym jak przygotowaliśmy wszystkie niezbędne szczegóły. Wystarczy wywołać metodę `trainer.train()`. Warto mieć na uwadze, że proces ten będzie jednak długotrwały - jedna epoka treningu na przygotowanych danych będzie trwała ponad 1 godzinę. Na szczęście, dzięki ustawieniu ewaluacji co 300 kroków, będziemy mogli obserwować jak model radzie sobie z postawionym przed nim problemem na danych ewaluacyjnych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "sULHvH_bMBmW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 3m @ 4080\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "kVp2hbadsEm5"
      },
      "source": [
        "## Zadanie 4 (0.5 punkt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "3kmxKtZp_VP6",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "Wybierz losową stronę z Wikipedii i skopiuj fragment tekstu do Notebook. Zadaj 3 pytania, na które można udzielić odpowiedź na podstawie tego fragmentu tekstu oraz 3 pytania, na które nie można udzielić odpowiedzi. Oceń jakość predykcji udzielanych przez model.\n",
        "\n",
        "Pamiętaj, aby przełączyć model w tryb inferencji (`model.eval()`). W przeciwnym razie wyniki będą losowe, ponieważ aktywny będzie mechanizm *dropoutu*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "Ou-a-tVoU9wG",
        "tags": [
          "ex"
        ]
      },
      "outputs": [],
      "source": [
        "# your_code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "4KcwGtp1xlDn",
        "tags": [
          "ex"
        ]
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "oJXK8qWCtoY-",
        "tags": []
      },
      "source": [
        "# Odpowiadanie na pytania\n",
        "\n",
        "Drugim problemem, którym zajmie się w tym laboratorium jest odpowiadanie na pytania. Zmierzymy się z wariantem tego problemu, w którym model sam formułuje odpowiedź, na podstawie pytania i kontekstu, w których znajduje się odpowiedź na pytanie (w przeciwieństwie do wariantu, w którym model wskazuje lokalizację odpowiedzi na pytanie)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "BmcULWcHsEnE"
      },
      "source": [
        "## Zadanie 5 (1 punkt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "SL3VibwXYdu2",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "Rozpocznij od przygotowania danych. Wybierzem tylko te pytania, które posiadają odpowiedź (`is_impossible=False`). Uwzględnij zarówno pytania *pewne* (pole `answers`) jak i *prawdopodobne* (pole `plausible_answers`). Wynikowy zbiór danych powinien mieć identyczną strukturę, jak w przypadku zadania z klasyfikacją, ale etykiety zamiast wartości 0 i 1, powinny zawierać odpowiedź na pytanie, a sama nazwa etykiety powinna być zmieniona z `label` na `labels`, w celu odzwierciedlenia faktu, że teraz zwracane jest wiele etykiet.\n",
        "\n",
        "Wyświetl liczbę danych (par: pytanie - odpowiedź) w zbiorze treningowym i zbiorze ewaluacyjnym.\n",
        "\n",
        "Opakuj również zbiory w klasy z biblioteki `datasets` i zapisz je na dysku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "auGRaK7x1vf9",
        "tags": [
          "ex"
        ]
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# your_code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "IsZe71D5FMhw",
        "tags": []
      },
      "source": [
        "Zanim przejdziemy do dalszej części, sprawdźmy, czy dane zostały poprawnie utworzone. Zweryfikujmy przede wszystkim, czy klucze `text` oraz `label` zawieraja odpowiednie wartości:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "ZN8Q0h7PF_aw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(datasets[\"train\"][0][\"text\"])\n",
        "print(datasets[\"train\"][0][\"labels\"])\n",
        "print(datasets[\"dev\"][0][\"text\"])\n",
        "print(datasets[\"dev\"][0][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "zLghVU7EEaHb",
        "tags": []
      },
      "source": [
        "Tokenizacja danych dla problemu odpowiadania na pytania jest nieco bardziej problematyczna. W pierwszej kolejności trzeba wziąć pod uwagę, że dane wynikowe (etykiety), też muszą podlegać tokenizacji. Realizowane jest to poprzez wywołanie tokenizera, z opcją `text_target` ustawioną na łańcuch, który ma być stokenizowany.\n",
        "\n",
        "Ponadto wcześniej nie przejmowaliśmy się za bardzo tym, czy wykorzystywany model obsługuje teksty o założonej długości. Teraz jednak ma to duże znaczenie. Jeśli użyjemy modelu, który nie jest w stanie wygenerować odpowiedzi o oczekiwanej długości, to nie możemy oczekiwać, że model ten będzie dawał dobre rezultaty dla danych w zbiorze treningowym i testowym.\n",
        "\n",
        "W pierwszej kolejności dokonamy więc tokenizacji bez ograniczeń co do długości tekstu. Ponadto, stokenizowane odpowiedzi przypiszemy do klucza `label`. Do tokenizacji użyjemy tokenizera stowarzyszonego z modelem  `allegro/plt5-base`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-12-22T10:30:09.564553Z",
          "start_time": "2022-12-22T10:30:09.155839Z"
        },
        "editable": true,
        "id": "WljAN9tMg5uU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "plt5_tokenizer = AutoTokenizer.from_pretrained(\"allegro/plt5-base\")\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = plt5_tokenizer(examples[\"text\"])\n",
        "    labels = plt5_tokenizer(text_target=examples[\"labels\"])\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "OlSHE98SIFjv",
        "tags": []
      },
      "source": [
        "Sprawdźmy jak dane wyglądają po tokenizacji:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "z3IM-Cd1IEba",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(tokenized_datasets[\"train\"][0].keys())\n",
        "print(tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
        "print(tokenized_datasets[\"train\"][0][\"labels\"])\n",
        "print(len(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
        "print(len(tokenized_datasets[\"train\"][0][\"labels\"]))\n",
        "example = tokenized_datasets[\"train\"][0]\n",
        "\n",
        "print(\"|\".join(plt5_tokenizer.convert_ids_to_tokens(list(example[\"input_ids\"]))))\n",
        "print(\"|\".join(plt5_tokenizer.convert_ids_to_tokens(list(example[\"labels\"]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "seBM6iumIY8x",
        "tags": []
      },
      "source": [
        "Wykorzystywany przez nas model ma złożoność pamięciową kwadratową ze względu na długość tekstu. Z tego względu chcemy ograniczyć długość danych wejściowych oraz tekstów podlegających predykcji.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "uFRUhdaSsEnx"
      },
      "source": [
        "## Zadanie 6 (0.5 punkt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "ytWAxKC3sEny"
      },
      "source": [
        "Stwórz histogramy prezentujące rozkład długości (jako liczby tokenów) tekstów wejściowych (`input_ids`) oraz odpowiedzi (`labels`) dla zbioru treningowego. Zinterpretuj (skomentuj) otrzymane wyniki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "wSg4cZ2Xw9fJ",
        "tags": [
          "ex"
        ]
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# your_code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "dyaT0ebG0InY",
        "tags": [
          "ex"
        ]
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "oTTrGUuvQQ63",
        "tags": []
      },
      "source": [
        "Przyjmiemy założenie, że teksty wejściowe będą miały maksymalnie 256 tokenów, a odpowiedzi do długości 32 tokenów.\n",
        "\n",
        "W poniższym kodzie uwzględniamy również fakt, że przy obliczaniu funkcji straty nie interesuje nas wliczanie tokenów wypełnienia (PAD), gdyż ich udział byłby bardzo duży, a nie wpływają one w żaden pozytywny sposób na ocenę poprawności działania modelu.\n",
        "\n",
        "Konteksty (pytanie + kontekst odpowiedzi) ograniczamy do 256 tokenów, ze wzgędu na ograniczenia pamięciowe (zajętość pamięci dla modelu jest proporcjonalna do kwadratu długości tekstu). Dla kontekstów nie używamy parametru `padding`, ponieważ w trakcie treningu użyjemy modułu, który automatycznie doda padding, tak żeby wszystkie sekewncje miały długość najdłuższego tekstu w ramach paczki (moduł ten to `DataCollatorForSeq2Seq`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "EpW4MNa1tGUV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    result = plt5_tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "    targets = plt5_tokenizer(\n",
        "        examples[\"labels\"], truncation=True, max_length=32, padding=True\n",
        "    )\n",
        "    target_ids = [\n",
        "        [(l if l != plt5_tokenizer.pad_token_id else -100) for l in e]\n",
        "        for e in targets[\"input_ids\"]\n",
        "    ]\n",
        "    result[\"labels\"] = target_ids\n",
        "    return result\n",
        "\n",
        "\n",
        "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "uCLIl_cIyRxH",
        "tags": []
      },
      "source": [
        "Następnie weryfkiujemy, czy przetworzone teksty mają poprawną postać."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "bQ9i4ApASNIL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(tokenized_datasets[\"train\"][0].keys())\n",
        "print(tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
        "print(tokenized_datasets[\"train\"][0][\"labels\"])\n",
        "print(len(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
        "print(len(tokenized_datasets[\"train\"][0][\"labels\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kJhhNj0sEn3"
      },
      "source": [
        "## Ładowanie modelu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "GEqhSrxLAwCH",
        "tags": []
      },
      "source": [
        "Dla problemu odpowiadania na pytania potrzebować będziemy innego pre-trenowanego modelu oraz innego przygotowania danych. Jako model bazowy wykrzystamy polski wariant modelu T5 - [plT5](https://huggingface.co/allegro/plt5-base). Model ten trenowany był w zadaniu *span corruption*, czyli zadani polegającym na usunięciu fragmentu tekstu. Model na wejściu otrzymywał tekst z pominiętymi pewnymi fragmentami, a na wyjściu miał odtwarzać te fragmenty. Oryginalny model T5 dodatkowo pretrenowany był na kilku konkretnych zadaniach z zakresu NLP (w tym odpowiadaniu na pytania). W wariancie plT5 nie przeprowadzono jednak takiego dodatkowego procesu.\n",
        "\n",
        "Poniżej ładujemy model dla zadania, w którym model generuje tekst na podstawie innego tekstu (tzn. jest to zadanie zamiany tekstu na tekst, po angielsku zwanego też *Sequence-to-Sequence*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "ZvEOsWlAiWOu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"allegro/plt5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzNEv8aDsEn5"
      },
      "source": [
        "W celu poprawy szybkości treningu  moglibyśmy użyć podobnej metody jak przy klasyfikacji. Istnieją jednak bardziej efektywne metody, np. low-rank adaptation (LoRA), które dokomponują macierze wag na dwie macietze o mniejszej liczbie parameyrów. Są one szczególnie istotne dla uczenia dużych modeli. Dzięki bibliotece PEFT ich użycie jest bardzo proste. Skorzystamy zatem z LoRA przy tworzeniu modelu QA.\n",
        "\n",
        "W pierwszej kolejności konfigurujemy metodę:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q32-48esEoS"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-_IPERqsEoT"
      },
      "source": [
        "Teraz opakowujemy oryginalny model w model PEFT. Oryginalny model nie będzie modyfikowany. Musimy jednak pamietać żeby wszędzie używać modelu PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MYY2ENcsEoU"
      },
      "outputs": [],
      "source": [
        "peft_model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(peft_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGb4YS1esEoW"
      },
      "source": [
        "Widzimy, że liczba modyfikowalnych parametrów jest bardzo mała względem oryginalnego modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "5UhNiDor4CSa",
        "tags": []
      },
      "source": [
        "## Trening modelu QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "8TWCljD_yb0E",
        "tags": []
      },
      "source": [
        "Ostatnim krokiem przed uruchomieniem treningu jest zdefiniowanie metryk, wskazujacych jak model radzi sobie z problemem. Wykorzystamy dwie metryki:\n",
        "* *exact match* - która sprawdza dokładne dopasowanie odpowiedzi do wartości referencyjnej, metryka ta jest bardzo restrykcyjna, ponieważ pojedynczy znak będzie powodował, że wartość będzie niepoprawna,\n",
        "* *blue score* - metryka uwzględniająca częściowe dopasowanie pomiędzy odpowiedzią a wartością referencyjną, najczęściej używana jest do oceny maszynowego tłumaczenia tekstu, ale może być również przydatna w ocenie wszelkich zadań, w których generowany jest tekst.\n",
        "\n",
        "Wykorzystujemy bibilotekę `evaluate`, która zawiera definicje obu metryk.\n",
        "\n",
        "Przy konwersji identyfikatorów tokenów na tekstu zamieniamy również z powroten tokeny o wartości -100 na identyfikatory paddingu. W przeciwnym razie dostaniemy błąd o nieistniejącym identyfikatorze tokenu.\n",
        "\n",
        "W procesie treningu pokazujemy również różnicę między jedną wygenerowaną oraz prawdziwą odpowiedzią dla zbioru ewaluacyjnego. W ten sposób możemy śledzić co rzeczywiście dzieje się w modelu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "bcjDjmjT2rVm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "exact = evaluate.load(\"exact_match\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.where(predictions != -100, predictions, plt5_tokenizer.pad_token_id)\n",
        "    decoded_preds = plt5_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, plt5_tokenizer.pad_token_id)\n",
        "    decoded_labels = plt5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    print(\"prediction: \" + decoded_preds[0])\n",
        "    print(\"reference : \" + decoded_labels[0])\n",
        "\n",
        "    result = exact.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {**result, **bleu.compute(predictions=decoded_preds, references=decoded_labels)}\n",
        "    del result[\"precisions\"]\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != plt5_tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "k6GfQTF8sEoZ"
      },
      "source": [
        "## Zadanie 7 (0.5 punkty)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "g_49SDmpy5yo",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "\n",
        "Korzystając z klasy Seq2SeqTrainingArguments zdefiniuj następujące parametry trenignu:\n",
        "* inny katalog z wynikami\n",
        "* liczba epok: 2\n",
        "* wielkość paczki: 24 (jeśli Twoja karta ma 16GB VRAM)\n",
        "* ewaluacja co 200 kroków,\n",
        "* szybkość uczenia: 1e-3\n",
        "* optymalizator: adafactor\n",
        "* maksymalna długość generowanej odpowiedzi: 32,\n",
        "* akumulacja wyników ewaluacji: 4\n",
        "* generowanie wyników podczas ewaluacji\n",
        "\n",
        "**W treningu nie używamy optymalizacji FP16!** Jej użycie spowoduje, że model nie będzie się trenował. Jeśli chcesz użyć optymalizacji, to możesz skorzystać z **BF16**.\n",
        "\n",
        "Argumenty powinny również wskazywać, że przeprowadzoany jest proces uczenia i ewaluacji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "t4fTGCQ5yWc-",
        "tags": [
          "ex"
        ]
      },
      "outputs": [],
      "source": [
        "# your_code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [
          "ex"
        ],
        "id": "OMUQwVclsEoc"
      },
      "source": [
        "## Zadanie 8 (0.5 punktu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "r1wc95I3zrEC",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "Utwórz obiekt trenujący `Seq2SeqTrainer`, za pomocą którego będzie trenowany model odpowiadający na pytania.\n",
        "\n",
        "Obiekt ten powinien:\n",
        "* wykorzystywać model `plt5-base`,\n",
        "* wykorzystywać zbiór `train` do treningu,\n",
        "* wykorzystawać zbiór `dev` do evaluacji,\n",
        "* wykorzystać klasę batchującą (`data_collator`) o nazwie `DataCollatorWithPadding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-12-20T14:05:20.769322Z",
          "start_time": "2022-12-20T14:05:20.344307Z"
        },
        "editable": true,
        "id": "X-l-Phk6zkvL",
        "tags": [
          "ex"
        ]
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding\n",
        "\n",
        "# your_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "30ng1TNCFoBM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir output_qa/runs # gdrive/MyDrive/poquad/output_qa/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "-pyrQ4m70WE6",
        "tags": []
      },
      "source": [
        "Mając przygotowane wszystkie dane wejściowe możemy rozpocząć proces treningu.\n",
        "\n",
        "**Uwaga**: proces treningu na Google Colab z wykorzystaniem akceleratora zajmuje ok. 1 godziny. Uruchomienie treningu na CPU może trwać ponad 1 dzień!\n",
        "\n",
        "Możesz pominąć ten proces i w kolejnych krokach wykorzystać gotowy model `apohllo/plt5-base-poquad`, który znajduje się w repozytorium Hugginface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "CVew4vRlhyVP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#14m @ 4080\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "L3-k_ctqvwmf",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "## Zadanie 9 (1.5 punkt)\n",
        "\n",
        "Korzystając z wywołania `generate` w modelu, wygeneruj odpowiedzi dla 1 kontekstu i 10 pytań dotyczących tego kontekstu. Pamiętaj aby zamienić identyfikatory tokenów na ich treść. Możesz do tygo wykorzystać wywołanie `decode` z tokenizera.\n",
        "\n",
        "Jeśli w poprzednim punkcie nie udało Ci się wytrenować modelu, możesz skorzystać z modelu `apohllo/plt5-base-poquad`.\n",
        "\n",
        "Oceń wyniki (odpowiedzi) generowane przez model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "a4BuKkoPbEtn",
        "tags": [
          "ex"
        ]
      },
      "outputs": [],
      "source": [
        "# your_code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FArrKA6E0ix3",
        "tags": [
          "ex"
        ]
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "S9mN-0PiFoBN",
        "tags": [
          "ex"
        ]
      },
      "source": [
        "# Zadanie dodatkowe (3 punkty)\n",
        "\n",
        "Stworzenie pełnego rozwiązania w zakresie odpowiadania na pytania wymaga również znajdowania kontekstów, w których może pojawić się pytanie.\n",
        "\n",
        "Obenie istnieje coraz więcej modeli neuronalnych, które bardzo dobrze radzą sobie ze znajdowaniem odpowiednich tekstów. Również dla języka polskiego następuje tutaj istotny postęp. Powstała m.in. [strona śledząca postępy w tym zakresie](https://huggingface.co/spaces/sdadas/pirb).\n",
        "\n",
        "Korzystając z informacji na tej stronie wybierz jeden z modeli do wyszukiwania kontekstów (najlepiej o rozmiarze `base` lub `small`). Zamień konteksty występujące w zbiorze PoQuAD na reprezentacje wektorowe. To samo zrób z pytaniami występującymi w tym zbiorze. Dla każdego pytania znajdź kontekst, który według modelu najlepiej odpowiada na zadane pytanie. Do znalezienia kontekstu oblicz iloczyn skalarny pomiędzy reprezentacją pytania oraz wszystkimi kontekstami ze zbioru. Następnie uruchom model generujący odpowiedź na znalezionym kontekście. Porównaj wyniki uzyskiwane w ten sposób, z wynikami, gdy poprawny kontekst jest znany.\n",
        "\n",
        "W celu przyspieszenie obliczeń możesz zmniejszyć liczbę pytań i odpowiadających im kontekstów. Pamiętaj jednak, żeby liczba kontekstów była odpowiednio duża (sugerowana wartość min. to 1000 kontekstów), tak żeby znalezienie kontekstu nie było trywialne.\n",
        "\n",
        "Zastanów się jakiej metryki użyjesz do oceny znalezionych odpowiedzi. Przedstaw wyniki oceny odpowiedzi pełnego rozwiązania według wybranej metryki."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "763px",
        "left": "10px",
        "top": "150px",
        "width": "294.188px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}